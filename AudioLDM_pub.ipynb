{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zDb5oYf3aChsGJaZ752dhmhP9NhZKs8w",
      "authorship_tag": "ABX9TyMWUdixH6daOuIw1m0KZDXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralTextToAudio/blob/main/AudioLDM_pub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">AudioLDM<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Text-to-audio</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralTextToAudio\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "Generate audio from text-prompt using [AudioLDM](https://github.com/haoheliu/AudioLDM).\n",
        "\n",
        "This notebook has been optimized to use both [Diffusers AudioLDM pipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm) (for text-to-audio generation) and native [AudioLDM Python API](https://github.com/haoheliu/AudioLDM) (for Audio-to-Audio and Style Transfer, incl. stereo simulation). This makes first setup slow, but usage fast (see tips below to make future usage considerably faster)."
      ],
      "metadata": {
        "id": "GBgr33OisX3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions and tips\n"
      ],
      "metadata": {
        "id": "sMmxZmNB8wds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### __Notebook usage__\n",
        "#### __General__\n",
        "- `mount_drive` is optional but highly recommended, as it enables you to auto-save all generated WAV files as well as used checkpoints directly to your Google Drive (and thus sync to your computer in near real-time, if you have Google Drive installed). Should you opt not to mount Google Drive, directory _faux_drive_ (`/content/faux_drive`) found in the Files browser of the Colab runtime works as if it was your _My Drive_. You may use it to upload/download files via Colab's own Files browser pretending it's your Google Drive.\n",
        "- All directory and file paths should be relative to your Google Drive root (My Drive). E.g. `output_dir` value should be `Music/AI-Generated-Sounds` if you have a directory called _Music_ in your Drive, containing a subdirectory called _AI-Generated-Sounds_. All paths are case-sensitive.\n",
        "- Checkpoint `audioldm-l-full` requires Premium GPU. Other checkpoints should work with standard GPU.\n",
        "- `local_models_dir` (optional) will save the used checkpoints in your Google Drive and/or use them from there if already available. Using this is a significant timesaver on Setup next times you use the notebook.\n",
        "- `output_dir` is where the generated WAV files will be saved.\n",
        "- `batch` will just repeat whatever you're generating that many times.\n",
        "- If seed is set to `0` (zero), a random seed will be used.\n",
        "- You may use a `;` (semicolon) in the prompt field as a separator, in which case a separate audio file will be generated for each semicolon-separated prompt in a single run.\n",
        "\n",
        "#### __Quality simulation__\n",
        "Some audio quality enhancements using traditional methods – unrelated to AudioLDM per se – have been added to this notebook. These settings will not by any means make the audio sound excellent, but perhaps slightly better than the default 16 kHz mono.\n",
        "- `stereo_width` (when greater than zero) will generate a secondary audio file applying the same prompt as low-strength Style Transfer on the initial generation, and mash these files as left and right channels with given width.\n",
        "- `convert_to_44khz` will convert the generated 16 kHz audio to 44.1 kHz audio __after__ generation, using FFMPEG with a decent interpolation filter. This can make some types of audio sound noticably better.\n",
        "- `simulate_high_end` will generate some high end (>10 kHz) by traditional methods using pitch shift and highpass filtering.\n",
        "\n",
        "#### __Audio-to-Audio generation__\n",
        "- Enter a file path to `init_audio_file` field.\n",
        "- Remove all text from `prompt` field.\n",
        "- Set `style_strength` to zero.\n",
        "\n",
        "#### __Style Transfer__ \n",
        "- Enter a file path to `init_audio_file` field.\n",
        "- Describe the style you want in `prompt` field.\n",
        "- Set `style_strength` to greater than zero.\n",
        "\n",
        "### __Prompt tips__\n",
        "Naturally a good prompt depends on what you're after, but generally:\n",
        "Consider adding more detailed description of what kind of sound you want (add adjectives, etc.).\n",
        "For better quality, you may try some additional keywords generally associated with better quality, for example\n",
        "- in studio\n",
        "- studio recording\n",
        "- high quality\n",
        "- album (for music)\n",
        "\n",
        "### __Advanced__\n",
        "If you are familiar with Python scripting and want to generate prompt lists programmatically (or just for clarity) you may set your prompts to `prompt_list = []` and use it by simply entering text value `prompt_list` to the `prompt` field. If it's a list of text prompts, it will be used in similar fashion as prompt field value containing semicolons. You may also include prompt-specific parameters in `prompt_list` by making it a list of lists, where each sublist is in format `['<prompt>', <seed>, <guidance_scale>, '<output_dir>']`"
      ],
      "metadata": {
        "id": "VLZ65d_q9Vzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "BFRLyEgz9cwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "\n",
        "# Print colors\n",
        "class c:\n",
        "  title = '\\033[96m'\n",
        "  ok = '\\033[92m'\n",
        "  okb = '\\033[94m'\n",
        "  warn = '\\033[93m'\n",
        "  fail = '\\033[31m'\n",
        "  endc = '\\033[0m'\n",
        "  bold = '\\033[1m'\n",
        "  dark = '\\33[90m'\n",
        "  u = '\\033[4m'\n",
        "\n",
        "def op(typex, msg, value='', time=False):\n",
        "  if time == True:\n",
        "    stamp = timestamp(human_readable=True)\n",
        "    typex = c.dark+stamp+' '+typex\n",
        "  if value != '':\n",
        "    print(typex+msg+c.endc, end=' ')\n",
        "    print(value)\n",
        "  else:\n",
        "    print(typex+msg+c.endc)\n",
        "\n",
        "quick_setup = False #@ param {type:\"boolean\"}\n",
        "\n",
        "use_diffusers = True\n",
        "use_github = True if quick_setup == False else False\n",
        "\n",
        "force_setup = False\n",
        "repositories = ['https://github.com/haoheliu/AudioLDM.git']\n",
        "apt_packages = 'ffmpeg'\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "skip_setup = False #@ param {type:\"boolean\"}\n",
        "local_models_dir = \"\" #@param {type:\"string\"}\n",
        "use_checkpoint = \"audioldm-m-full\" #@param [\"audioldm-s-full\", \"audioldm-l-full\", \"audioldm-s-full-v2\",\"audioldm-m-text-ft\", \"audioldm-s-text-ft\", \"audioldm-m-full\"]\n",
        "\n",
        "if '-text-' in use_checkpoint:\n",
        "  use_diffusers = False\n",
        "\n",
        "pip_packages = 'transformers diffusers accelerate' if use_diffusers == True else ''\n",
        "\n",
        "if quick_setup == True:\n",
        "  op(c.title, 'Performing quick setup')\n",
        "\n",
        "if use_github == False:\n",
        "  repositories = []\n",
        "  apt_packages = ''\n",
        "  if local_models_dir != '':\n",
        "    op(c.fail, '!! local_models_dir is ignored on quick setup')\n",
        "  local_models_dir = ''\n",
        "\n",
        "ckpt_urls = {\n",
        "  \"audioldm-s-full\": \"https://zenodo.org/record/7600541/files/audioldm-s-full?download=1\",\n",
        "  \"audioldm-l-full\": \"https://zenodo.org/record/7698295/files/audioldm-full-l.ckpt?download=1\",\n",
        "  \"audioldm-s-full-v2\": \"https://zenodo.org/record/7698295/files/audioldm-full-s-v2.ckpt?download=1\",\n",
        "  \"audioldm-m-text-ft\": \"https://zenodo.org/record/7813012/files/audioldm-m-text-ft.ckpt?download=1\",\n",
        "  \"audioldm-s-text-ft\": \"https://zenodo.org/record/7813012/files/audioldm-s-text-ft.ckpt?download=1\",\n",
        "  \"audioldm-m-full\": \"https://zenodo.org/record/7813012/files/audioldm-m-full.ckpt?download=1\"\n",
        "}\n",
        "\n",
        "ckpt_url = ckpt_urls[use_checkpoint]\n",
        "use_ckpt = ckpt_url.split('files/')[1].split('?')[0]\n",
        "\n",
        "import os\n",
        "from google.colab import output, files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "if pip_packages != '':\n",
        "  !pip -q install {pip_packages}\n",
        "if apt_packages != '':\n",
        "  !apt-get update && apt-get install {apt_packages}\n",
        "\n",
        "if use_diffusers == True:\n",
        "  import torch\n",
        "  from diffusers import AudioLDMPipeline\n",
        "  from transformers import AutoProcessor, ClapModel\n",
        "\n",
        "  # make Space compatible with CPU duplicates\n",
        "  if torch.cuda.is_available():\n",
        "      device = \"cuda\"\n",
        "      torch_dtype = torch.float16\n",
        "  else:\n",
        "      device = \"cpu\"\n",
        "      torch_dtype = torch.float32\n",
        "\n",
        "  # load the diffusers pipeline\n",
        "  if use_checkpoint == 'audioldm-s-full':\n",
        "    repo_id = \"cvssp/audioldm\"\n",
        "  else:\n",
        "    repo_id = \"cvssp/\" + use_checkpoint\n",
        "  pipe = AudioLDMPipeline.from_pretrained(repo_id, torch_dtype=torch_dtype).to(device)\n",
        "  pipe.unet = torch.compile(pipe.unet)\n",
        "\n",
        "  # CLAP model (only required for automatic scoring)\n",
        "  clap_model = ClapModel.from_pretrained(\"sanchit-gandhi/clap-htsat-unfused-m-full\").to(device)\n",
        "  processor = AutoProcessor.from_pretrained(\"sanchit-gandhi/clap-htsat-unfused-m-full\")\n",
        "  generator = torch.Generator(device)\n",
        "\n",
        "import sys, time, ntpath, string, random, librosa, librosa.display, IPython, shutil, math, psutil, datetime, requests, pytz\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from datetime import timedelta\n",
        "\n",
        "def gen_id(type='short'):\n",
        "  id = ''\n",
        "  if type == 'timestamp':\n",
        "    id = timestamp()\n",
        "  if type == 'short':\n",
        "    id = requests.get('https://api.inha.asia/k/?type=short').text\n",
        "  if type == 'long':\n",
        "    id = requests.get('https://api.inha.asia/k').text\n",
        "  return id\n",
        "\n",
        "def timestamp(no_slash=False, human_readable=False, helsinki_time=True, date_only=False):\n",
        "  if helsinki_time == True:\n",
        "    dt = datetime.datetime.now(pytz.timezone('Europe/Helsinki'))\n",
        "  else:\n",
        "    dt = datetime.datetime.now()\n",
        "  if no_slash == True:\n",
        "    dt = dt.strftime(\"%Y%m%d%H%M%S\")\n",
        "  else:\n",
        "    if human_readable == True:\n",
        "      dt = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    else:\n",
        "      if date_only == True:\n",
        "        dt = dt.strftime(\"%Y-%m-%d\")\n",
        "      else:\n",
        "        dt = dt.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "  return dt;\n",
        "\n",
        "def fix_path(path, add_slash=False):\n",
        "  if path.endswith('/'):\n",
        "    path = path #path[:-1]\n",
        "  if not path.endswith('/'):\n",
        "    path = path+\"/\"\n",
        "  if path.startswith('/') and add_slash == True:\n",
        "    path = path[1:]\n",
        "  return path\n",
        "  \n",
        "def path_leaf(path):\n",
        "  head, tail = ntpath.split(path)\n",
        "  return tail or ntpath.basename(head)\n",
        "\n",
        "def path_dir(path):\n",
        "  return path.replace(path_leaf(path), '')\n",
        "\n",
        "def path_ext(path, only_ext=False):\n",
        "  filename, extension = os.path.splitext(path)\n",
        "  if only_ext == True:\n",
        "    extension = extension[1:]\n",
        "  return extension\n",
        "\n",
        "def basename(path):\n",
        "  filename = os.path.basename(path).strip()#.replace(\" \", \"_\")\n",
        "  filebase = os.path.splitext(filename)[0]\n",
        "  return filebase\n",
        "\n",
        "def slug(s):\n",
        "  valid_chars = \"-_. %s%s\" % (string.ascii_letters, string.digits)\n",
        "  file = ''.join(c for c in s if c in valid_chars)\n",
        "  file = file.replace(' ','_')\n",
        "  return file\n",
        "  \n",
        "def fetch(url, save_as):\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
        "  try:\n",
        "    r = requests.get(url, stream=True, headers=headers, timeout=5)\n",
        "    if r.status_code == 200:\n",
        "      with open(save_as, 'wb') as f:\n",
        "        r.raw.decode_content = True\n",
        "        shutil.copyfileobj(r.raw, f)\n",
        "      resp = r.status_code\n",
        "    else:\n",
        "      resp = 0\n",
        "  except requests.exceptions.ConnectionError as e:\n",
        "    r = 0\n",
        "    resp = r\n",
        "  return resp\n",
        "\n",
        "def list_audio(path, midi=False):\n",
        "  audiofiles = []\n",
        "  for ext in ('*.wav', '*.aiff', '*.aif', '*.caf' '*.flac', '*.mp3', '*.m4a', '*.ogg', '*.WAV', '*.AIFF', '*.AIF', '*.CAF', '*.FLAC', '*.MP3', '*.OGG'):\n",
        "    audiofiles.extend(glob(join(path, ext)))\n",
        "  if midi is True:\n",
        "    for ext in ('*.mid', '*.midi', '*.MID', '*.MIDI'):\n",
        "      audiofiles.extend(glob(join(path, ext)))\n",
        "  audiofiles.sort()\n",
        "  return audiofiles\n",
        "\n",
        "def audio_player(input, sr=44100, limit_duration=2):\n",
        "  if type(input) != np.ndarray:\n",
        "    input, sr = librosa.load(input, sr=None, mono=False)\n",
        "  if limit_duration > 0:\n",
        "    last_sample = math.floor(limit_duration*60*sr)\n",
        "    if input.shape[-1] > last_sample:\n",
        "      input = input[:last_sample, :last_sample]\n",
        "      op(c.warn, 'WARN! Playback of below audio player is limited to first '+str(limit_duration)+' minutes to prevent Colab from crashing.\\n')\n",
        "  IPython.display.display(IPython.display.Audio(input, rate=sr))\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive == True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive/'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  drive_root = '/content/faux_drive/'\n",
        "  if not os.path.isdir(drive_root):\n",
        "    os.mkdir(drive_root)\n",
        "  \n",
        "\n",
        "if mount_drive == False:\n",
        "  local_models_dir = ''\n",
        "\n",
        "if len(repositories) > 0 and skip_setup == False:\n",
        "  for repo in repositories:\n",
        "    %cd /content/\n",
        "    install_dir = fix_path('/content/'+path_leaf(repo).replace('.git', ''))\n",
        "    repo = repo if '.git' in repo else repo+'.git'\n",
        "    !git clone {repo}\n",
        "    if os.path.isfile(install_dir+'setup.py') or os.path.isfile(install_dir+'setup.cfg'):\n",
        "      !pip install -e {install_dir}\n",
        "    if os.path.isfile(install_dir+'requirements.txt'):\n",
        "      !pip install -r {install_dir}/requirements.txt\n",
        "\n",
        "if len(repositories) == 1:\n",
        "  %cd {install_dir}\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "if not os.path.isdir(dir_tmp): os.mkdir(dir_tmp)\n",
        "\n",
        "use_ckpt_path = os.path.expanduser('~')+'/.cache/audioldm/'\n",
        "\n",
        "if not os.path.isdir(use_ckpt_path):\n",
        "  os.makedirs(use_ckpt_path)\n",
        "\n",
        "if os.path.isfile(use_ckpt_path+use_ckpt):\n",
        "  op(c.ok, 'Checkpoint found:', use_ckpt)\n",
        "else:\n",
        "  if local_models_dir != '':\n",
        "    models_dir = drive_root+fix_path(local_models_dir)\n",
        "    if not os.path.isdir(models_dir):\n",
        "      os.makedirs(models_dir)\n",
        "    # for ckpt_url in ckpt_urls:\n",
        "    #   use_ckpt = ckpt_url.split('files/')[1].split('?')[0]\n",
        "    if os.path.isfile(models_dir+use_ckpt):\n",
        "      op(c.title, 'Fetching local ckpt:', models_dir.replace(drive_root, '')+use_ckpt)\n",
        "      shutil.copy(models_dir+use_ckpt, use_ckpt_path+use_ckpt)\n",
        "      op(c.ok, 'Done.')\n",
        "    else:\n",
        "      op(c.warn, 'Downloading '+use_ckpt+' to ', models_dir.replace(drive_root, ''))\n",
        "      !wget {ckpt_url} -O {models_dir}{use_ckpt}\n",
        "      shutil.copy(models_dir+use_ckpt, use_ckpt_path+use_ckpt)\n",
        "      op(c.ok, 'Done.')\n",
        "  else:\n",
        "    # for ckpt_url in ckpt_urls:\n",
        "    #   use_ckpt = ckpt_url.split('files/')[1].split('?')[0]\n",
        "    if quick_setup == False:\n",
        "      models_dir = use_ckpt_path\n",
        "      op(c.warn, 'Downloading', use_ckpt)\n",
        "      !wget {ckpt_url} -O {models_dir}{use_ckpt}\n",
        "      # shutil.copy(models_dir+use_ckpt, use_ckpt_path+use_ckpt)\n",
        "      op(c.ok, 'Done.')\n",
        "    else:\n",
        "      op(c.warn, 'Skipping AudioLDM checkpoints...')\n",
        "\n",
        "if use_github:\n",
        "  _ckpt_path = use_ckpt_path+use_ckpt\n",
        "  op(c.title, 'Build model', _ckpt_path)\n",
        "  sys.path.append('/content/AudioLDM/audioldm/')\n",
        "  from audioldm import text_to_audio, style_transfer, super_resolution_and_inpainting, build_model, latent_diffusion\n",
        "  audioldm = build_model(ckpt_path=_ckpt_path, model_name=use_checkpoint)\n",
        "\n",
        "def round_to_multiple(number, multiple):\n",
        "  x = multiple * round(number / multiple)\n",
        "  if x == 0: x = multiple\n",
        "  return x\n",
        "\n",
        "def text2audioDiffusers(text, negative_prompt, duration, guidance_scale, random_seed, n_candidates):\n",
        "  waveforms = pipe(\n",
        "      text,\n",
        "      audio_length_in_s=duration,\n",
        "      guidance_scale=guidance_scale,\n",
        "      negative_prompt=negative_prompt,\n",
        "      num_waveforms_per_prompt=n_candidates if n_candidates else 1,\n",
        "      generator=generator.manual_seed(int(random_seed)),\n",
        "  )[\"audios\"]\n",
        "\n",
        "  if waveforms.shape[0] > 1:\n",
        "      waveform = score_waveforms(text, waveforms)\n",
        "  else:\n",
        "      waveform = waveforms[0]\n",
        "  return waveform\n",
        "\n",
        "def score_waveforms(text, waveforms):\n",
        "  inputs = processor(text=text, audios=list(waveforms), return_tensors=\"pt\", padding=True)\n",
        "  inputs = {key: inputs[key].to(device) for key in inputs}\n",
        "  with torch.no_grad():\n",
        "      logits_per_text = clap_model(**inputs).logits_per_text  # this is the audio-text similarity score\n",
        "      probs = logits_per_text.softmax(dim=-1)  # we can take the softmax to get the label probabilities\n",
        "      most_probable = torch.argmax(probs)  # and now select the most likely audio waveform\n",
        "  waveform = waveforms[most_probable]\n",
        "  return waveform\n",
        "  \n",
        "def text2audio(text, duration, audio_path, guidance_scale, random_seed, n_candidates, steps):\n",
        "  waveform = text_to_audio(\n",
        "    audioldm,\n",
        "    text,\n",
        "    audio_path,\n",
        "    random_seed,\n",
        "    duration=duration,\n",
        "    guidance_scale=guidance_scale,\n",
        "    ddim_steps=steps,\n",
        "    n_candidate_gen_per_text=int(n_candidates)\n",
        "  )\n",
        "  if(len(waveform) == 1):\n",
        "    waveform = waveform[0]\n",
        "  return waveform\n",
        "\n",
        "def styleaudio(text, duration, audio_path, strength, guidance_scale, random_seed, steps):\n",
        "  waveform = style_transfer(\n",
        "    audioldm,\n",
        "    text,\n",
        "    audio_path,\n",
        "    strength,\n",
        "    random_seed,\n",
        "    duration=duration,\n",
        "    guidance_scale=guidance_scale,\n",
        "    ddim_steps=steps,\n",
        "  )\n",
        "  if(len(waveform) == 1):\n",
        "    waveform = waveform[0]\n",
        "  return waveform\n",
        "\n",
        "\n",
        "# time_mask_ratio_start_and_end=(0.10, 0.15), # regenerate the 10% to 15% of the time steps in the spectrogram\n",
        "# time_mask_ratio_start_and_end=(1.0, 1.0), # no inpainting\n",
        "# freq_mask_ratio_start_and_end=(0.75, 1.0), # regenerate the higher 75% to 100% mel bins\n",
        "# freq_mask_ratio_start_and_end=(1.0, 1.0), # no super-resolution\n",
        "def superres(text, duration, audio_path, guidance_scale, random_seed, n_candidates, steps):\n",
        "  waveform = super_resolution_and_inpainting(\n",
        "    audioldm,\n",
        "    text,\n",
        "    audio_path,\n",
        "    random_seed,\n",
        "    ddim_steps=steps,\n",
        "    duration=duration,\n",
        "    guidance_scale=guidance_scale,\n",
        "    n_candidate_gen_per_text=n_candidates,\n",
        "    freq_mask_ratio_start_and_end=(0.75, 1.0)\n",
        "  )\n",
        "  if(len(waveform) == 1):\n",
        "    waveform = waveform[0]\n",
        "  return waveform\n",
        "\n",
        "def narrow_stereo(left_data, right_data, amount):\n",
        "  left = left_data * amount + right_data * (1-amount)\n",
        "  right = right_data * amount + left_data * (1-amount)\n",
        "  return np.array([left, right])\n",
        "\n",
        "def extract_prompt(file_path):\n",
        "  bn = basename(wav)\n",
        "  parts = bn.split('_')\n",
        "  prompt = []\n",
        "  for i, part in enumerate(parts):\n",
        "    if i > 0 and not part.replace('.','').isnumeric() and part != '':\n",
        "      prompt.append(part)\n",
        "    elif len(part) > 6 and part.isnumeric():\n",
        "      seed = int(part)\n",
        "    elif '.' in part:\n",
        "      guidance_scale = float(part)\n",
        "  prompt = ' '.join(prompt)\n",
        "  return [prompt, seed, guidance_scale]\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "\n",
        "def highpass_audio(input, cutoff=10000, fs=44100, order=6):\n",
        "  if type(input) == np.ndarray:\n",
        "    data = input\n",
        "    sr = fs\n",
        "  else:\n",
        "    data, sr = librosa.load(input, sr=None, mono=False)\n",
        "  b, a = butter(order, cutoff, fs=fs, btype='highpass', analog=False)\n",
        "  y = lfilter(b, a, data)\n",
        "  return y\n",
        "\n",
        "def apply_highpass_mix(input, hipass_vol=0.5, mix_vol=0.8, cutoff=10000, fx=44100, order=6):\n",
        "  if type(input) == np.ndarray:\n",
        "    data = input\n",
        "    sr = fs\n",
        "  else:\n",
        "    data, sr = librosa.load(input, sr=None, mono=False)\n",
        "  octave_up = librosa.effects.pitch_shift(data, sr=sr, n_steps=12, res_type='soxr_vhq') * hipass_vol\n",
        "  hipass = highpass_audio(octave_up, cutoff, fx, order)\n",
        "  return data*mix_vol+hipass\n",
        "\n",
        "def applyInhaCSS():\n",
        "  display(IPython.display.HTML('''\n",
        "    <style type=\"text/css\">\n",
        "      :root {\n",
        "        --bg-color: #eee;\n",
        "        --fg-color: #333;\n",
        "        --radius: 4px;\n",
        "        --border: 0;\n",
        "        --bold: 600;\n",
        "        --dimmed: #ccc;\n",
        "        --darkened: #aaa;\n",
        "        --v-margin: 10px;\n",
        "      }\n",
        "      input.inhads,\n",
        "      button.inhads {\n",
        "        display: inline-block;\n",
        "        border: var(--border);\n",
        "        border-radius: var(--radius);\n",
        "        background: var(--bg-color);\n",
        "        color: var(--fg-color);\n",
        "        margin-top: var(--v-margin);\n",
        "        margin-bottom: calc(var(--v-margin) * 2);\n",
        "      }\n",
        "      input.inhads {\n",
        "        padding: 10px 15px;\n",
        "        min-width: 30%;\n",
        "      }\n",
        "      input.inhads.input-with-button {\n",
        "        border-top-left-radius: var(--radius);\n",
        "        border-bottom-left-radius: var(--radius);\n",
        "        border-top-right-radius: 0;\n",
        "        border-bottom-right-radius: 0;\n",
        "      }\n",
        "      button.inhads {\n",
        "        cursor: pointer;\n",
        "      }\n",
        "      button.inhads.input-button {\n",
        "        display: inline-block;\n",
        "        margin: 0;\n",
        "        padding: 4.5px 10px;\n",
        "        position: relative;\n",
        "        top: 3px;\n",
        "        font-size: 20px;\n",
        "        border-top-left-radius: 0;\n",
        "        border-bottom-left-radius: 0;\n",
        "        border-top-right-radius: var(--radius);\n",
        "        border-bottom-right-radius: var(--radius)\n",
        "      }\n",
        "      button.inhads.download-button {\n",
        "        padding: 10px 15px;\n",
        "        font-weight: var(--bold);\n",
        "      }\n",
        "      button.inhads.download-button.done {\n",
        "        background: var(--dimmed);\n",
        "      }\n",
        "      button.inhads.download-button.done::before {\n",
        "        content: '✔ ';\n",
        "        color: #080;\n",
        "      }\n",
        "      .inhads.input-note {\n",
        "        padding-left: 10px;\n",
        "        display: none;\n",
        "        font-weight: var(--bold);\n",
        "      }\n",
        "      .inhads.disabled {\n",
        "        color: #666;\n",
        "        background: var(--darkened);\n",
        "      }\n",
        "    </style>\n",
        "  '''))\n",
        "\n",
        "def dl_btn(file_path, show_path=False, show_filename=False):\n",
        "  applyInhaCSS()\n",
        "  id = rnd_str(8)\n",
        "  view_path = file_path if show_path else path_leaf(file_path) if show_filename else ''\n",
        "  def download_file():\n",
        "    files.download(file_path)\n",
        "  display(IPython.display.HTML('''\n",
        "    <button class=\"inhads download-button\" id=\"btn_'''+id+'''\">Download</button> '''+view_path+'''\n",
        "    <script>\n",
        "      document.querySelector(\"#btn_'''+id+'''\").onclick = () => {\n",
        "        let btn = document.querySelector(\"#btn_'''+id+'''\");\n",
        "        google.colab.kernel.invokeFunction(\"notebook.download'''+id+'''\", [], {});\n",
        "        btn.innerHTML='Downloading...';\n",
        "        btn.classList.add('disabled');\n",
        "        btn.disabled=true;\n",
        "        setTimeout(() => {\n",
        "          btn.innerHTML='Downloaded';\n",
        "          btn.classList.remove('disabled');\n",
        "          btn.classList.add('done');\n",
        "          btn.disabled=false;\n",
        "        }, 2000);\n",
        "      };\n",
        "    </script>\n",
        "  '''))\n",
        "  output.register_callback('notebook.download'+id, download_file)\n",
        "\n",
        "def rnd_str(length):\n",
        "  letters = string.ascii_lowercase\n",
        "  result_str = ''.join(random.choice(letters) for i in range(length))\n",
        "  return result_str\n",
        "\n",
        "prompt_list = []\n",
        "first_generated = False\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "print()\n",
        "op(c.title, 'Using:', use_ckpt, time=True)\n",
        "op(c.ok, 'Setup finished.', time=True)\n",
        "print()\n"
      ],
      "metadata": {
        "id": "-l7u34qYZ07a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate audio"
      ],
      "metadata": {
        "id": "IrR0JORR9rre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@title # Generate audio\n",
        "\n",
        "start_from = 1\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "negative_prompt = \"low quality, average quality, snare\" #@param {type:\"string\"}\n",
        "\n",
        "output_dir = \"\" #@param {type:\"string\"}\n",
        "duration = 10 #@param {type:\"slider\", min:2.5, max:30, step:2.5}\n",
        "guidance_scale = 2.5 #@param {type:\"slider\", min:2, max:5, step:0.5}\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "candidates = 3 #@param {type:\"slider\", min:2, max:5, step:1}\n",
        "batch = 1 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <b>Style Transfer & Audio-to-Audio</b> settings – Ignore these settings if you just want to generate audio by text prompt.\n",
        "init_audio_file = \"\" #@param {type:\"string\"}\n",
        "style_strength = 0 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <b>Quality simulation</b> by traditional methods (unrelated to AudioLDM)\n",
        "convert_to_44khz = True #@param {type:\"boolean\"}\n",
        "stereo_width = 0.65 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "keep_16k = True #@ param {type:\"boolean\"}\n",
        "simulate_high_end = 0.25 #@param {type:\"slider\", min:0, max:0.5, step:0.05}\n",
        "display_players = True #@ param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# what_to_do = \"Audio-to-audio generation\" #@param [\"Audio-to-audio generation\", \"Super-resolution\", \"Style Transfer\"]\n",
        "\n",
        "what_to_do = None\n",
        "superresolution = False #@ param {type:\"boolean\"}\n",
        "\n",
        "trunc = 150\n",
        "settings_in_filename = False\n",
        "\n",
        "if what_to_do == 'Audio-to-audio-generation': action = 'audio2audio'\n",
        "if what_to_do == 'Super-resolution': action = 'superres'\n",
        "if what_to_do == 'Style Transfer': action = 'style'\n",
        "if what_to_do == 'Inpaint': action = 'inpaint'\n",
        "\n",
        "ddim_steps = 200\n",
        "og_seed = seed\n",
        "og_duration = duration\n",
        "uniq_id = gen_id()\n",
        "sr = 16000\n",
        "\n",
        "# Prompt/input\n",
        "if ';' in prompt:\n",
        "  inputs = prompt.split(';')\n",
        "elif prompt == 'prompt_list':\n",
        "  inputs = prompt_list\n",
        "else:\n",
        "  inputs = [prompt]\n",
        "\n",
        "if isinstance(inputs[0], list):\n",
        "  inputs = [x.strip() for x in inputs]\n",
        "\n",
        "# Output\n",
        "if output_dir == '':\n",
        "  if mount_drive is True:\n",
        "    dir_out = dir_tmp\n",
        "  if mount_drive is False:\n",
        "    dir_out = drive_root+'generated-audio/'\n",
        "    if not os.path.isdir(dir_out):\n",
        "      os.mkdir(dir_out)\n",
        "else:\n",
        "  if not os.path.isdir(drive_root+output_dir):\n",
        "    os.makedirs(drive_root+output_dir)\n",
        "  dir_out = drive_root+fix_path(output_dir)\n",
        "\n",
        "if convert_to_44khz == True and keep_16k == True:\n",
        "  dir_16k = dir_out+'16khz/'\n",
        "  if not os.path.isdir(dir_16k):\n",
        "    os.mkdir(dir_16k)\n",
        "    \n",
        "og_dir_out = dir_out\n",
        "if batch == 0: batch = 1  \n",
        "inputs = inputs * batch\n",
        "\n",
        "timer_start = time.time()\n",
        "total = len(inputs)\n",
        "action = 'generate'\n",
        "init_path = None\n",
        "\n",
        "op(c.title, 'Run ID', uniq_id, time=True)\n",
        "\n",
        "for i, input in enumerate(inputs, start_from):\n",
        "\n",
        "  dir_out = og_dir_out\n",
        "\n",
        "  if not i % 10:\n",
        "    output.clear()\n",
        "    op(c.warn, 'Cell output is cleared every 10 generations to keep Colab running smoothly.', time=True)\n",
        "    op(c.warn, 'You can find all audio files from directory', dir_out.replace(drive_root, ''), time=True)\n",
        "    print()\n",
        "    op(c.title, 'Run ID', uniq_id, time=True)\n",
        "    \n",
        "  prompt = input\n",
        "  \n",
        "  predefined_file_out = ''\n",
        "\n",
        "  if isinstance(input, list):\n",
        "    op(c.warn, 'Prompt-specific parameters found, ignoring form values.', time=True)\n",
        "    prompt = input[0]\n",
        "    seed = int(input[1])\n",
        "    og_seed = seed\n",
        "    guidance_scale = float(input[2])\n",
        "    outd = input[3]\n",
        "    if len(input) > 3:\n",
        "      predefined_file_out = input[4]\n",
        "    if outd != '':\n",
        "      if not os.path.isdir(dir_out+outd):\n",
        "        os.mkdir(dir_out+outd)\n",
        "      dir_out = dir_out+outd+'/'\n",
        "  \n",
        "  ndx_info = str(i)+'/'+str(total)+' '\n",
        "  print()\n",
        "\n",
        "  if os.path.isfile(dir_out+predefined_file_out):\n",
        "    op(c.warn, ndx_info+'Already exists, skipping', predefined_file_out)\n",
        "    continue\n",
        "\n",
        "  if init_audio_file != '':\n",
        "    if os.path.isfile(drive_root+init_audio_file):\n",
        "      init_path = drive_root+init_audio_file\n",
        "      if superresolution is True:\n",
        "        action = 'superres'\n",
        "      elif style_strength > 0:\n",
        "        init_filename = path_leaf(init_path)\n",
        "        op(c.title, ndx_info+'Styling audio:', init_path.replace(drive_root, ''), time=True)\n",
        "        op(c.title, 'With prompt:', prompt, time=True)\n",
        "        action = 'style'\n",
        "      else:\n",
        "        op(c.title, ndx_info+'Audio-to-audio generation:', init_path.replace(drive_root, ''), time=True)\n",
        "        # op(c.title, 'With prompt:', prompt, time=True)\n",
        "        prompt = None\n",
        "        action = 'audio2audio'\n",
        "      # Trim duration if init duration is shorter than given duration\n",
        "      init_y, init_sr = librosa.load(init_path, sr=None, mono=True)\n",
        "      init_duration = librosa.get_duration(init_y, init_sr)\n",
        "      duration = round_to_multiple(init_duration, 2.5) if init_duration < og_duration else duration\n",
        "      \n",
        "    else:\n",
        "      op(c.fail, ndx_info+'Init audio file not found!', time=True)\n",
        "      sys.exit('Make sure init_audio_file is a valid audio file and a valid file path relative to your My Drive.')\n",
        "  else:\n",
        "    op(c.title, ndx_info+'Generating audio:', prompt, time=True)\n",
        "\n",
        "    if first_generated == False:\n",
        "      op(c.warn, 'First generation takes a short while to start. Next generations will be considerably faster.', time=True)\n",
        "      print()\n",
        "      first_generated = True\n",
        "\n",
        "    if isinstance(input, list):\n",
        "      print('File:', path_leaf(predefined_file_out))\n",
        "      print('Using seed:', seed)\n",
        "      print('Using guidance scale:', guidance_scale)\n",
        "\n",
        "  if og_seed == 0: seed = int(time.time()) - 1229904000 + random.randint(11111111, 99999999)\n",
        "\n",
        "  \n",
        "  addn = str(seed)+'_'+str(guidance_scale)+'_' if settings_in_filename == True else ''\n",
        "  fo_head = dir_out+uniq_id+'_'+str(i).zfill(3)+'__'+addn\n",
        "\n",
        "  if action == 'generate':\n",
        "    if predefined_file_out != '':\n",
        "      file_out = dir_out+predefined_file_out\n",
        "    else:\n",
        "      file_out = fo_head+slug(prompt)[:trunc]+'.wav'\n",
        "\n",
        "    if use_diffusers == True:\n",
        "      generated_audio = text2audioDiffusers(prompt, negative_prompt, duration, guidance_scale, seed, candidates)\n",
        "    else:\n",
        "      generated_audio = text2audio(prompt, duration, None, guidance_scale, seed, candidates, ddim_steps)\n",
        "\n",
        "\n",
        "  elif action == 'audio2audio':\n",
        "    file_out = fo_head+basename(init_path)+'.wav'\n",
        "    generated_audio = text2audio('placeholder', duration, init_path, guidance_scale, seed, candidates, ddim_steps)\n",
        "  elif action == 'superres':\n",
        "    file_out = fo_head+basename(init_path)+'.wav'\n",
        "    y, sr = librosa.load(init_path, sr=None)\n",
        "    duration = librosa.get_duration(y, sr=sr)\n",
        "    if duration > 30: duration = 30\n",
        "    generated_audio = superres(None, duration, init_path, guidance_scale, seed, candidates, ddim_steps)\n",
        "  elif action == 'style':\n",
        "    file_out = fo_head+basename(init_path)+'_'+slug(prompt)[:trunc]+'.wav'\n",
        "    generated_audio = styleaudio(prompt, duration, init_path, style_strength, guidance_scale, seed, ddim_steps)\n",
        "  else:\n",
        "    op(c.fail, 'Something went wrong.')\n",
        "    sys.exit()\n",
        "\n",
        "  if stereo_width > 0:\n",
        "    op(c.okb, 'Working on stereo...')\n",
        "    lefty = dir_tmp+'left.wav'\n",
        "    righto = dir_tmp+'right.wav'\n",
        "    sf.write(lefty, generated_audio.T, sr, subtype='PCM_16')\n",
        "    style_strength = 0.15\n",
        "    left_channel, init_sr = librosa.load(lefty, sr=None, mono=True)\n",
        "    left_duration = librosa.get_duration(left_channel, sr)\n",
        "    duration = round_to_multiple(left_duration, 2.5)\n",
        "    right_channel = styleaudio(prompt, duration, lefty, style_strength, guidance_scale, seed, ddim_steps)\n",
        "    sf.write(righto, right_channel.T, sr, subtype='PCM_16')\n",
        "    left_channel, _ = librosa.load(lefty, sr=None)\n",
        "    right_channel, _ = librosa.load(righto, sr=None)\n",
        "    last_sample = min([len(left_channel), len(right_channel)])\n",
        "    generated_audio = narrow_stereo(left_channel[:last_sample], right_channel[:last_sample], stereo_width)\n",
        "\n",
        "  if convert_to_44khz == True:\n",
        "    op(c.okb, 'Converting to 44.1 kHz...')\n",
        "    tmp_16k = dir_tmp+'16k.wav'\n",
        "    tmp_44k = dir_tmp+'44k.wav'\n",
        "    sf.write(tmp_16k, generated_audio.T, sr, subtype='PCM_24')\n",
        "    !ffmpeg -hide_banner -loglevel panic -i \"{tmp_16k}\" -ar 44100 \"{tmp_44k}\"\n",
        "    if simulate_high_end > 0:\n",
        "      op(c.okb, 'Simulating high-end...')\n",
        "      hpm = apply_highpass_mix(tmp_44k, hipass_vol=simulate_high_end, mix_vol=0.8, cutoff=11000)\n",
        "      sf.write(file_out, hpm.T, 44100, subtype='PCM_24')\n",
        "      os.remove(tmp_44k)\n",
        "    else:\n",
        "      shutil.copy(tmp_44k, file_out)\n",
        "      os.remove(tmp_44k)\n",
        "    if keep_16k == True:\n",
        "      shutil.copy(tmp_16k, file_out.replace(dir_out, dir_16k))\n",
        "    os.remove(tmp_16k)\n",
        "  else:\n",
        "    sf.write(file_out, generated_audio.T, sr, subtype='PCM_24')\n",
        "\n",
        "  if os.path.isfile(file_out):\n",
        "    if display_players: audio_player(file_out, sr=sr)\n",
        "    print()\n",
        "    if output_dir == '':\n",
        "      dl_btn(file_out, show_filename=True)\n",
        "    else:\n",
        "      op(c.ok, 'Saved as', file_out.replace(drive_root, ''), time=True)\n",
        "  else:\n",
        "    op(c.fail, 'Error saving', file_out.replace(drive_root, ''), time=True)  \n",
        "\n",
        "# -- END THINGS --\n",
        "\n",
        "timer_end = time.time()\n",
        "\n",
        "print()\n",
        "op(c.okb, 'Elapsed', timedelta(seconds=timer_end-timer_start), time=True)\n",
        "op(c.ok, 'FIN.')"
      ],
      "metadata": {
        "id": "Znu4P-Gtsdrr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}