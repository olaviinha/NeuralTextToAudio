{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UYVdwqGUR0AGaDoWB6eNIJuIrrwhyV5L",
      "authorship_tag": "ABX9TyM1J7IejSJs5qRwUzow5adw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralTextToAudio/blob/main/tortoise_tts_pub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Tortoise TTS<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Text to spoken word audio</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralTextToAudio\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "- All file and directory paths should be relative to your Google Drive root (_My Drive_). E.g. `voice_audio` value should be `Audio/test-voice.wav`, if you have a directory called _Audio_ in your drive, and you want to use _test-voice.wav_ from that directory. Paths are case-sensitive.\n",
        "- This notebook will attempt to prepare a coherent voice dataset from `voice_audio` input, but optimal `voice_audio` for coherent output should be a path to a WAV file of about 1 minute in duration, or a directory containing a total of about 1 minute of WAV files.\n",
        "- In case `voice_audio` contents exceeds 1 minute considerably, random clips (from random file, or files depending on contents, if directory given) will be picked for voice cloning."
      ],
      "metadata": {
        "id": "GBgr33OisX3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "\n",
        "force_setup = False\n",
        "repositories = ['https://github.com/neonbjb/tortoise-tts.git']\n",
        "pip_packages = 'scipy transformers==4.19.0'\n",
        "apt_packages = 'sox'\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "skip_setup = False #@ param {type:\"boolean\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  if apt_packages != '':\n",
        "    !apt-get update && apt-get install {apt_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive == True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if len(repositories) > 0 and skip_setup == False:\n",
        "  for repo in repositories:\n",
        "    %cd /content/\n",
        "    install_dir = fix_path('/content/'+path_leaf(repo).replace('.git', ''))\n",
        "    repo = repo if '.git' in repo else repo+'.git'\n",
        "    !git clone {repo}\n",
        "    if os.path.isfile(install_dir+'requirements.txt'):\n",
        "      !pip install -r {install_dir}/requirements.txt\n",
        "    if os.path.isfile(install_dir+'setup.py') or os.path.isfile(install_dir+'setup.cfg'):\n",
        "      !pip install -e {install_dir}\n",
        "\n",
        "if len(repositories) == 1:\n",
        "  %cd {install_dir}\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "dir_tmp_corpus = '/content/tmp/corpus/'\n",
        "dir_tmp_slices = '/content/tmp/slices/'\n",
        "dir_tmp_clips = '/content/tmp/clips/'\n",
        "dir_tmp_processed = '/content/tmp/processed/'\n",
        "create_dirs([dir_tmp, dir_tmp_corpus, dir_tmp_slices, dir_tmp_clips, dir_tmp_processed])\n",
        "\n",
        "import time, sys\n",
        "from datetime import timedelta\n",
        "import math\n",
        "\n",
        "# Imports used through the rest of the notebook.\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import IPython\n",
        "import librosa\n",
        "import soundfile as sf \n",
        "\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "\n",
        "def slice_to_frames(audio_data, slice_duration, fade_in=0, fade_out=0, sr=44100):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  clips = math.ceil(a_duration/slice_duration)\n",
        "  frames = []\n",
        "  for i in range(clips-1):\n",
        "    if i > 0 and i < clips:\n",
        "      start = i*slice_duration\n",
        "      audio_clip = clip_audio(audio_data, start, slice_duration)\n",
        "      if fade_in > 0 or fade_out > 0:\n",
        "        audio_clip = fade_audio(audio_clip, fade_in, fade_out, sr=sr)\n",
        "      frames.append(audio_clip)\n",
        "  return frames\n",
        "\n",
        "def clip_audio(audio_data, start, duration, sr=44100):\n",
        "  xstart = librosa.time_to_samples(start, sr=sr)\n",
        "  xduration = librosa.time_to_samples(start+duration, sr=sr)\n",
        "  audio_data = audio_data[:, xstart:xduration]\n",
        "  return audio_data\n",
        "\n",
        "def fade_audio(audio_data, fade_in=0.05, fade_out=0.05, sr=44100):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  if fade_in > 0:\n",
        "    fade_in_to = librosa.time_to_samples(fade_in, sr=sr)\n",
        "    in_y = audio_data[:, 0:fade_in_to]\n",
        "    fade_ins = []\n",
        "    for channel in in_y:\n",
        "      fade = [ i/len(channel)*smp for i, smp in enumerate(channel) ]\n",
        "      fade_ins.append(fade)\n",
        "    fade_ins = np.array(fade_ins)\n",
        "    tail_start = fade_in_to+1  \n",
        "    tail = audio_data[:, tail_start:]\n",
        "    audio_data = np.concatenate([fade_ins, tail], axis=1)\n",
        "  if fade_out > 0:\n",
        "    fade_out_start = librosa.time_to_samples(a_duration-fade_out, sr=sr)\n",
        "    out_y = audio_data[:, fade_out_start:]\n",
        "    fade_outs = []\n",
        "    for channel in out_y:\n",
        "      fade = [ smp-(i/len(channel)*smp) for i, smp in enumerate(channel) ]\n",
        "      fade_outs.append(fade)\n",
        "    fade_outs = np.array(fade_outs)\n",
        "    head_start = fade_out_start-1\n",
        "    head = audio_data[:, :head_start]\n",
        "    audio_data = np.concatenate([head, fade_outs], axis=1)\n",
        "  return audio_data\n",
        "\n",
        "def remove_silence(audio, window_size=0.2, threshold=0.1, save_as='', sr=44100):\n",
        "  if type(audio) != np.ndarray:\n",
        "    y, sr = librosa.load(audio, sr=None, mono=False)\n",
        "  else:\n",
        "    y = audio\n",
        "  audio_slices = slice_to_frames(y, window_size, sr=sr)\n",
        "  silence_removed_list = []\n",
        "  for audio_slice in audio_slices:\n",
        "    if max(audio_slice[0]) > threshold or max(audio_slice[1]) < -abs(threshold):\n",
        "      silence_removed_list.append(audio_slice)\n",
        "  silence_removed = np.concatenate(silence_removed_list, axis=1)\n",
        "  if save_as != '':\n",
        "    sf.write(save_as, silence_removed.T, sr)\n",
        "    return save_as\n",
        "  return silence_removed\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "def get_audio_duration(file):\n",
        "  y, sr = librosa.load(voice_file, sr=None, mono=True)\n",
        "  return librosa.get_duration(y, sr=sr)\n",
        "\n",
        "def get_dir_size(dir_path='.'):\n",
        "  total_size = 0\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    for f in filenames:\n",
        "      fp = os.path.join(dirpath, f)\n",
        "      if not os.path.islink(fp):\n",
        "        total_size += os.path.getsize(fp)\n",
        "  return total_size\n",
        "\n",
        "def chop_to_sentences(text):\n",
        "  delimiter = '.'\n",
        "  temp = [e+delimiter for e in text.split(delimiter) if e]\n",
        "  sentences = []\n",
        "  for sentence in temp:\n",
        "    delimiter = '?'\n",
        "    if delimiter in sentence:\n",
        "      wtf = sentence.split(delimiter)\n",
        "      for f in wtf:\n",
        "        if f[-1] != '.' and f[-1] != '?' and f[-1] != '?':\n",
        "          f = f+'?'\n",
        "        if f != '':\n",
        "          sentences.append(f.strip())\n",
        "    elif sentence.strip() != '' and len(sentence.strip()) > 1:\n",
        "      sentences.append(sentence.strip())\n",
        "  return sentences\n",
        "\n",
        "# This will download all the models used by Tortoise from the HuggingFace hub.\n",
        "tts = TextToSpeech()\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.', time=True)"
      ],
      "metadata": {
        "id": "Zl44n6FXsbnY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Generate spoken word audio\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "voice_audio = \"\" #@param {type:\"string\"}\n",
        "combo_voice = False #@ param {type:\"boolean\"}\n",
        "preset = \"high_quality\" #@param [\"standard\", \"fast\", \"ultra_fast\", \"high_quality\"]\n",
        "output_dir = \"\" #@param {type:\"string\"}\n",
        "end_session_when_done = False #@ param {type: \"boolean\"}\n",
        "\n",
        "save_txt = True\n",
        "timer_start = time.time()\n",
        "uniq_id = gen_id()\n",
        "\n",
        "\n",
        "slice_length = 12 # seconds per slice\n",
        "use_slices = 5 # slices to use\n",
        "optimal_samples_duration = slice_length * use_slices # total duration\n",
        "sample_rate = 24000\n",
        "chunk_sentences = 2 # process this many sentences in one go\n",
        "dir_byte_limit = 48000000\n",
        "merge_sentences = True\n",
        "\n",
        "op(c.title, 'Run ID:', uniq_id, time=True)\n",
        "print()\n",
        "\n",
        "voice_corpus = voice_audio\n",
        "prompts = chop_to_sentences(text)\n",
        "\n",
        "if chunk_sentences > 1:\n",
        "  prompts = [''.join(prompts[i:i+chunk_sentences]) for i in range(0, len(prompts), chunk_sentences)]\n",
        "\n",
        "clean_dirs([dir_tmp_corpus, dir_tmp_slices, dir_tmp_clips, dir_tmp_processed])\n",
        "\n",
        "if os.path.isfile(drive_root+voice_corpus):\n",
        "  clean_dirs([dir_tmp_corpus])\n",
        "  shutil.copy(drive_root+voice_corpus, dir_tmp_corpus)\n",
        "  voice_dirs = [dir_tmp_corpus]\n",
        "else:\n",
        "  if voice_corpus == 'voice_list':\n",
        "    voice_dirs = [drive_root+x for x in voice_list]\n",
        "  elif ',' in voice_corpus:\n",
        "    voice_dirs = [drive_root+fix_path(x.strip()) for x in voice_corpus.split(',')]\n",
        "  elif ';' in voice_corpus:\n",
        "    voice_dirs = [drive_root+fix_path(x.strip()) for x in voice_corpus.split(';')]\n",
        "  else:\n",
        "    voice_dirs = [drive_root+fix_path(voice_corpus)]\n",
        "\n",
        "# Output\n",
        "if output_dir == '':\n",
        "  if mount_drive == True:\n",
        "    dir_out = dir_tmp\n",
        "  else:\n",
        "    dir_out = drive_root\n",
        "else:\n",
        "  if not os.path.isdir(drive_root+output_dir):\n",
        "    os.mkdir(drive_root+output_dir)\n",
        "  dir_out = drive_root+fix_path(output_dir)\n",
        "\n",
        "total = len(voice_dirs * len(prompts))\n",
        "use_voices = []\n",
        "\n",
        "txt_file = dir_out+uniq_id+'.txt'\n",
        "if save_txt: append_txt(txt_file, timestamp(human_readable=True)+' '+uniq_id+'\\n\\n'+text+'\\n\\n'+'combo_voice: '+str(combo_voice)+'\\n'+'preset: '+preset+'\\n'+'dir_out: '+dir_out+'\\n\\n')\n",
        "\n",
        "for i, voice_dir in enumerate(voice_dirs, 1):\n",
        "  if voice_dir == dir_tmp_corpus:\n",
        "    voice_name = basename(voice_corpus)\n",
        "  else:\n",
        "    voice_name = path_leaf(voice_dir)\n",
        "\n",
        "  use_voices.append(voice_name)\n",
        "  new_voice_dir = '/content/tortoise-tts/tortoise/voices/'+voice_name+'/'\n",
        "  if not os.path.isdir(new_voice_dir):\n",
        "    os.mkdir(new_voice_dir)\n",
        "  else:\n",
        "    clean_dirs([new_voice_dir])\n",
        "  voice_files = list_audio(voice_dir)\n",
        "\n",
        "  random.shuffle(voice_files)\n",
        "\n",
        "  if save_txt: append_txt(txt_file, voice_name+'\\n'+'In: '+voice_dir)\n",
        "\n",
        "  if len(voice_files) == 0:\n",
        "    print()\n",
        "    op(c.fail, 'Skipping '+voice_name+' - Reason: WAV files not found in dir:', voice_dir.replace(drive_root, ''), time=True)\n",
        "    if save_txt: append_txt(txt_file, 'Out: - (no wav found, SKIP)\\n')\n",
        "  else:\n",
        "    op(c.okb, 'Processing voice files...', time=True)\n",
        "    bytes_collected = 0\n",
        "    for voice_file in voice_files:\n",
        "      voice_file = remove_silence(voice_file, window_size=2, threshold=0.1, save_as=dir_tmp_processed+path_leaf(voice_file))\n",
        "      file_duration = get_audio_duration(voice_file)\n",
        "      slice_file = dir_tmp_slices+path_leaf(voice_file)\n",
        "\n",
        "      if file_duration > slice_length:\n",
        "        !sox {sox_q} \"{voice_file}\" -r 22050 {slice_file} trim 0 {slice_length} : newfile : restart\n",
        "      else:\n",
        "        !sox {sox_q} \"{voice_file}\" -r 22050 {slice_file}\n",
        "\n",
        "      clips = list_audio(dir_tmp_slices)\n",
        "\n",
        "      short_clips = []\n",
        "      long_clips = []\n",
        "      for clip in clips:\n",
        "        clip_duration = get_audio_duration(clip)\n",
        "        if clip_duration >= slice_length:\n",
        "          long_clips.append(clip)\n",
        "        else:\n",
        "          short_clips.append(clip)\n",
        "        if (len(long_clips)*slice_length >= optimal_samples_duration):\n",
        "          break\n",
        "      \n",
        "      if len(long_clips) >= use_slices:\n",
        "        selected_clips = random.sample(long_clips, use_slices)\n",
        "      else:\n",
        "        selected_clips = clips\n",
        "\n",
        "      if save_txt: append_txt(txt_file, 'Selected clips:')\n",
        "      for clip in selected_clips:\n",
        "        if save_txt: append_txt(txt_file, path_leaf(clip)+'\\n')\n",
        "        shutil.copy(clip, new_voice_dir)\n",
        "\n",
        "      file_size = os.path.getsize(voice_file)\n",
        "      bytes_collected += file_size\n",
        "      if bytes_collected > dir_byte_limit:\n",
        "        break\n",
        "    \n",
        "    merge_list = []\n",
        "    for ii, text in enumerate(prompts, 1):\n",
        "\n",
        "      ndx_info = str(i*ii)+'/'+str(total)+' '  \n",
        "\n",
        "      voice_samples = None\n",
        "      conditioning_latents = None\n",
        "      gen = None\n",
        "      \n",
        "      print()\n",
        "      op(c.title, ndx_info+'Processing', voice_name, time=True)\n",
        "\n",
        "      if combo_voice == False:\n",
        "        op(c.title, ndx_info+'Synthesizing', text+'...', time=True)\n",
        "\n",
        "        file_out = dir_out+uniq_id+'__'+voice_name+'_'+str(ii).zfill(3)+'_'+slug(text[:60])+'.wav'\n",
        "        if save_txt: append_txt(txt_file, 'Out: '+file_out+'\\n')\n",
        "        voice_samples, conditioning_latents = load_voice(voice_name)\n",
        "        gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n",
        "        torchaudio.save(file_out, gen.squeeze(0).cpu(), sample_rate)\n",
        "        if os.path.isfile(file_out): \n",
        "          op(c.ok, 'Saved', file_out.replace(drive_root, ''), time=True)\n",
        "          merge_list.append(file_out)\n",
        "        else:\n",
        "          op(c.fail, 'Error saving', file_out.replace(drive_root, ''), time=True)\n",
        "\n",
        "        del voice_samples\n",
        "        del conditioning_latents\n",
        "        del gen\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      import gc\n",
        "      gc.collect()\n",
        "\n",
        "    if merge_sentences == True:\n",
        "      sox_input_list = ' '.join(merge_list)\n",
        "      sox_merge_out = dir_out+uniq_id+'__'+voice_name+'_FULL.wav'\n",
        "      !sox {sox_q} {sox_input_list} {sox_merge_out}\n",
        "\n",
        "# if combo_voice == True:\n",
        "#   for text in prompts:\n",
        "#     print()\n",
        "#     op(c.title, 'Synthesizing', text[:40]+'...', time=True)\n",
        "#     file_out = dir_out+uniq_id+'__'+voice_name+'_'+slug(text[:60])+'.wav'\n",
        "#     if save_txt == True:\n",
        "#       append_txt(txt_file, 'Out: '+file_out+'\\n')\n",
        "#     voice_samples, conditioning_latents = load_voices(use_voices)\n",
        "#     gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n",
        "#     torchaudio.save(file_out, gen.squeeze(0).cpu(), sample_rate)\n",
        "#     # IPython.display.Audio(file_out)\n",
        "\n",
        "#     del voice_samples\n",
        "#     del conditioning_latents\n",
        "#     del gen\n",
        "#     # del tts\n",
        "#     torch.cuda.empty_cache()\n",
        "#     import gc\n",
        "#     gc.collect()\n",
        "\n",
        "\n",
        "timer_end = time.time()\n",
        "\n",
        "print()\n",
        "\n",
        "if save_txt: append_txt(txt_file, str(timedelta(seconds=timer_end-timer_start)) )\n",
        "if save_txt: append_txt(txt_file, 'Finished at '+timestamp(human_readable=True))\n",
        "\n",
        "op(c.okb, 'Elapsed', timedelta(seconds=timer_end-timer_start), time=True)\n",
        "op(c.ok, 'FIN.')\n",
        "\n",
        "if end_session_when_done is True: end_session()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DjMTvst0z2ng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}